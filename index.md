---
site: sandpaper::sandpaper_site
---

This lesson covers various strategies for deploying Generative AI models locally on your laptop or workstation. In particular the lesson will cover how to deploy and utilize Generative AI models on your laptop or workstation using the following tools.

1. [LLaMA C++](https://github.com/ggerganov/llama.cpp): Enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.
2. [LlamaFile](https://github.com/Mozilla-Ocho/llamafile): Make open-source LLMs more accessible to both developers and end users. Combines [LLaMA C++](https://github.com/ggerganov/llama.cpp) with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses all the complexity of LLMs down to a single-file executable (called a "llamafile") that runs locally on most computers, with no installation.
3. [Ollama](https://ollama.com) ([GitHub](https://github.com/ollama/ollama)): Get up and running with Llama 3, Mistral, Gemma 2, and other large language models. Uses [LLaMA C++](https://github.com/ggerganov/llama.cpp) as the backend.
4. [Open WebUI](https://openwebui.com) ([GitHub](https://github.com/open-webui/open-webui)): Extensible, self-hosted interface for AI that adapts to your workflow, all while operating entirely offline.
5. [Jupyter AI](https://github.com/jupyterlab/jupyter-ai): A generative AI extension for JupyterLab.
